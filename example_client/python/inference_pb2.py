# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: inference.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import builder as _builder
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


import onnx_pb2 as onnx__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n\x0finference.proto\x12\tinference\x1a\nonnx.proto\"D\n\x11InferenceRequest2\x12\r\n\x05shape\x18\x01 \x03(\x03\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\x12\x12\n\nmodel_name\x18\x03 \x01(\t\"\"\n\x12InferenceResponse2\x12\x0c\n\x04\x64\x61ta\x18\x01 \x03(\x02\"(\n\tinputInfo\x12\r\n\x05shape\x18\x01 \x03(\x03\x12\x0c\n\x04\x64\x61ta\x18\x02 \x01(\x0c\"\x18\n\x16PrintStatisticsRequest\"\x19\n\x17PrintStatisticsResponse\"I\n\x10InferenceRequest\x12!\n\x06tensor\x18\x01 \x03(\x0b\x32\x11.onnx.TensorProto\x12\x12\n\nmodel_name\x18\x03 \x01(\t\"6\n\x11InferenceResponse\x12!\n\x06tensor\x18\x01 \x03(\x0b\x32\x11.onnx.TensorProto\"\xa9\x01\n\x0bModelConfig\x12#\n\x05input\x18\x01 \x03(\x0b\x32\x14.onnx.ValueInfoProto\x12$\n\x06output\x18\x02 \x03(\x0b\x32\x14.onnx.ValueInfoProto\x12\x11\n\tbatch_dim\x18\x03 \x01(\x03\x12\x16\n\x0emax_batch_size\x18\x04 \x01(\x03\x12$\n\x1cmax_batch_delay_microseconds\x18\x05 \x01(\x03\x32\xb8\x01\n\x10InferenceService\x12H\n\tInference\x12\x1b.inference.InferenceRequest\x1a\x1c.inference.InferenceResponse\"\x00\x12Z\n\x0fPrintStatistics\x12!.inference.PrintStatisticsRequest\x1a\".inference.PrintStatisticsResponse\"\x00\x62\x06proto3')

_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'inference_pb2', globals())
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  _INFERENCEREQUEST2._serialized_start=42
  _INFERENCEREQUEST2._serialized_end=110
  _INFERENCERESPONSE2._serialized_start=112
  _INFERENCERESPONSE2._serialized_end=146
  _INPUTINFO._serialized_start=148
  _INPUTINFO._serialized_end=188
  _PRINTSTATISTICSREQUEST._serialized_start=190
  _PRINTSTATISTICSREQUEST._serialized_end=214
  _PRINTSTATISTICSRESPONSE._serialized_start=216
  _PRINTSTATISTICSRESPONSE._serialized_end=241
  _INFERENCEREQUEST._serialized_start=243
  _INFERENCEREQUEST._serialized_end=316
  _INFERENCERESPONSE._serialized_start=318
  _INFERENCERESPONSE._serialized_end=372
  _MODELCONFIG._serialized_start=375
  _MODELCONFIG._serialized_end=544
  _INFERENCESERVICE._serialized_start=547
  _INFERENCESERVICE._serialized_end=731
# @@protoc_insertion_point(module_scope)
